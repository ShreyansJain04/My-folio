[
  {
    "objectID": "unet.html",
    "href": "unet.html",
    "title": "shreyansjain04.github.io",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport os\nfrom glob import glob\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\n\n\nclass Dataset(torch.utils.data.Dataset):\n    \n      def __init__(\n          self,\n          transform = None,\n          classes = None,\n          augmentation = None\n          ):\n        self.imgpath_list = sorted(glob('C:/Users/asus/Desktop/ML/archive (1)/dataset/semantic_drone_dataset/original_images/*.jpg'))\n\n        self.labelpath_list = sorted(glob('C:/Users/asus/Desktop/ML/archive (1)/dataset/semantic_drone_dataset/label_images_semantic/*.png'))\n\n      def __getitem__(self, i):\n                \n        imgpath = self.imgpath_list[i]\n        img = cv2.imread(imgpath)\n        img = cv2.resize(img, dsize = (256, 256))\n        img = img / 255\n        img = torch.from_numpy(img.astype(np.float32)).clone()\n        img = img.permute(2, 0, 1)\n\n        labelpath = self.labelpath_list[i]\n        label = Image.open(labelpath)\n        label = np.asarray(label)\n        label = cv2.resize(label, dsize = (256, 256))\n        label = torch.from_numpy(label.astype(np.float32)).clone()\n        label = torch.nn.functional.one_hot(label.long(), num_classes = 24)\n        label = label.to(torch.float32)\n        label = label.permute(2, 0, 1)\n\n        data = {\"img\": img, \"label\": label}\n        return data\n\n      def __len__(self):\n            return len(self.imgpath_list)\n\n\ndataset = Dataset()\n\n\nlen(dataset)\n\n400\n\n\n\ntrain, val, test = torch.utils.data.random_split(dataset=dataset, lengths=[320, 40, 40], generator=torch.Generator().manual_seed(42))\n\n\nbatch_size = 8\n\ntrain_loader = torch.utils.data.DataLoader(train, batch_size, shuffle = True, drop_last = True)\nval_loader = torch.utils.data.DataLoader(val, batch_size)\ntest_loader = torch.utils.data.DataLoader(test, batch_size)\n\n\nimport torch\nimport torch.nn as nn\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UNet, self).__init__()\n\n        # Encoder\n        self.encoder_conv1 = self.conv_block(in_channels, 64)\n        self.encoder_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder_conv2 = self.conv_block(64, 128)\n        self.encoder_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder_conv3 = self.conv_block(128, 256)\n        self.encoder_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder_conv4 = self.conv_block(256, 512)\n        self.encoder_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Bottleneck\n        self.bottleneck_conv = self.conv_block(512, 1024)\n\n        # Decoder\n        self.decoder_upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.decoder_conv4 = self.conv_block(1024, 512)\n        self.decoder_upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.decoder_conv3 = self.conv_block(512, 256)\n        self.decoder_upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.decoder_conv2 = self.conv_block(256, 128)\n        self.decoder_upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.decoder_conv1 = self.conv_block(128, 64)\n\n        # Output layer\n        self.output_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        # Encoder\n        encoder1 = self.encoder_conv1(x)\n        encoder2 = self.encoder_conv2(self.encoder_pool1(encoder1))\n        encoder3 = self.encoder_conv3(self.encoder_pool2(encoder2))\n        encoder4 = self.encoder_conv4(self.encoder_pool3(encoder3))\n\n        # Bottleneck\n        bottleneck = self.bottleneck_conv(self.encoder_pool4(encoder4))\n\n        # Decoder\n        decoder4 = self.decoder_upconv4(bottleneck)\n        decoder4 = torch.cat((encoder4, decoder4), dim=1)\n        decoder4 = self.decoder_conv4(decoder4)\n        decoder3 = self.decoder_upconv3(decoder4)\n        decoder3 = torch.cat((encoder3, decoder3), dim=1)\n        decoder3 = self.decoder_conv3(decoder3)\n        decoder2 = self.decoder_upconv2(decoder3)\n        decoder2 = torch.cat((encoder2, decoder2), dim=1)\n        decoder2 = self.decoder_conv2(decoder2)\n        decoder1 = self.decoder_upconv1(decoder2)\n        decoder1 = torch.cat((encoder1, decoder1), dim=1)\n        decoder1 = self.decoder_conv1(decoder1)\n\n        # Output layer\n        output = self.output_conv(decoder1)\n\n        return output\n\n    def conv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n\n# Create an instance of the UNet model\nin_channels = 3  # Number of input channels (e.g., RGB image)\nout_channels = 24  # Number of output channels (e.g., number of classes)\nunet_model = UNet(in_channels, out_channels)\n\n\n# Print the model architecture\nprint(unet_model)\n\n# Move the model to the device (e.g., GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nunet_model = unet_model.to(device)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(unet_model.parameters(), lr=0.001)\n\n\n\nUNet(\n  (encoder_conv1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (encoder_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv2): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (encoder_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv3): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (encoder_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv4): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (encoder_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (bottleneck_conv): Sequential(\n    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (decoder_upconv4): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n  (decoder_conv4): Sequential(\n    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (decoder_upconv3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n  (decoder_conv3): Sequential(\n    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (decoder_upconv2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n  (decoder_conv2): Sequential(\n    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (decoder_upconv1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n  (decoder_conv1): Sequential(\n    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (output_conv): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1))\n)\n\n\n\nnum_epochs = 20\nbest_loss = float('inf')\nbest_model_state_dict = None\n\nfor epoch in range(num_epochs):\n    unet_model.train()\n    total_loss = 0.0\n\n    for i, sample in enumerate(train_loader):\n        inputs = sample['img'].to(device)\n        labels = sample['label'].to(device)\n        optimizer.zero_grad()\n        outputs = unet_model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n        if (i + 1) % 10 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n    average_loss = total_loss / len(train_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}\")\n\n    # Val\n    unet_model.eval()\n    total_val_loss = 0.0\n\n    for i, val_sample in enumerate(val_loader):\n        val_inputs = val_sample['img'].to(device)\n        val_labels = val_sample['label'].to(device)\n        val_outputs = unet_model(val_inputs)\n        val_loss = criterion(val_outputs, val_labels)\n        total_val_loss += val_loss.item()\n\n    average_val_loss = total_val_loss / len(val_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {average_val_loss:.4f}\")\n\n    # Check if the current model has the best val loss\n    if average_val_loss &lt; best_loss:\n        best_loss = average_val_loss\n        best_model_state_dict = unet_model.state_dict()\n\n# After training, you can use the best model state dict to load the best model\nif best_model_state_dict is not None:\n    unet_model.load_state_dict(best_model_state_dict)\n\nEpoch [1/20], Batch [10/40], Loss: 3.0880\nEpoch [1/20], Batch [20/40], Loss: 3.1385\nEpoch [1/20], Batch [30/40], Loss: 3.0065\nEpoch [1/20], Batch [40/40], Loss: 2.9891\nEpoch [1/20], Average Loss: 4.1308\nEpoch [1/20], Val Loss: 2.9962\nEpoch [2/20], Batch [10/40], Loss: 2.6857\nEpoch [2/20], Batch [20/40], Loss: 2.3428\nEpoch [2/20], Batch [30/40], Loss: 2.0463\nEpoch [2/20], Batch [40/40], Loss: 2.2359\nEpoch [2/20], Average Loss: 2.4777\nEpoch [2/20], Val Loss: 2.1007\nEpoch [3/20], Batch [10/40], Loss: 2.1799\nEpoch [3/20], Batch [20/40], Loss: 1.8913\nEpoch [3/20], Batch [30/40], Loss: 2.0913\nEpoch [3/20], Batch [40/40], Loss: 2.0040\nEpoch [3/20], Average Loss: 2.0722\nEpoch [3/20], Val Loss: 2.0068\nEpoch [4/20], Batch [10/40], Loss: 2.2464\nEpoch [4/20], Batch [20/40], Loss: 1.8623\nEpoch [4/20], Batch [30/40], Loss: 1.8600\nEpoch [4/20], Batch [40/40], Loss: 1.6405\nEpoch [4/20], Average Loss: 1.9816\nEpoch [4/20], Val Loss: 1.9437\nEpoch [5/20], Batch [10/40], Loss: 1.8653\nEpoch [5/20], Batch [20/40], Loss: 2.1128\nEpoch [5/20], Batch [30/40], Loss: 2.0028\nEpoch [5/20], Batch [40/40], Loss: 1.6816\nEpoch [5/20], Average Loss: 1.9182\nEpoch [5/20], Val Loss: 1.7616\nEpoch [6/20], Batch [10/40], Loss: 1.8177\nEpoch [6/20], Batch [20/40], Loss: 1.8627\nEpoch [6/20], Batch [30/40], Loss: 1.5637\nEpoch [6/20], Batch [40/40], Loss: 1.5862\nEpoch [6/20], Average Loss: 1.7076\nEpoch [6/20], Val Loss: 1.5132\nEpoch [7/20], Batch [10/40], Loss: 1.4824\nEpoch [7/20], Batch [20/40], Loss: 1.6678\nEpoch [7/20], Batch [30/40], Loss: 1.6311\nEpoch [7/20], Batch [40/40], Loss: 1.4797\nEpoch [7/20], Average Loss: 1.5647\nEpoch [7/20], Val Loss: 1.4779\nEpoch [8/20], Batch [10/40], Loss: 1.2996\nEpoch [8/20], Batch [20/40], Loss: 1.5209\nEpoch [8/20], Batch [30/40], Loss: 1.4417\nEpoch [8/20], Batch [40/40], Loss: 1.3309\nEpoch [8/20], Average Loss: 1.4776\nEpoch [8/20], Val Loss: 1.3638\nEpoch [9/20], Batch [10/40], Loss: 1.4993\nEpoch [9/20], Batch [20/40], Loss: 1.5758\nEpoch [9/20], Batch [30/40], Loss: 1.0758\nEpoch [9/20], Batch [40/40], Loss: 1.5663\nEpoch [9/20], Average Loss: 1.4572\nEpoch [9/20], Val Loss: 1.3545\nEpoch [10/20], Batch [10/40], Loss: 1.2293\nEpoch [10/20], Batch [20/40], Loss: 1.5350\nEpoch [10/20], Batch [30/40], Loss: 1.3701\nEpoch [10/20], Batch [40/40], Loss: 1.3719\nEpoch [10/20], Average Loss: 1.4138\nEpoch [10/20], Val Loss: 1.3394\nEpoch [11/20], Batch [10/40], Loss: 1.1866\nEpoch [11/20], Batch [20/40], Loss: 1.6845\nEpoch [11/20], Batch [30/40], Loss: 1.5186\nEpoch [11/20], Batch [40/40], Loss: 1.8439\nEpoch [11/20], Average Loss: 1.3889\nEpoch [11/20], Val Loss: 1.2776\nEpoch [12/20], Batch [10/40], Loss: 1.7524\nEpoch [12/20], Batch [20/40], Loss: 1.2105\nEpoch [12/20], Batch [30/40], Loss: 1.4042\nEpoch [12/20], Batch [40/40], Loss: 1.4318\nEpoch [12/20], Average Loss: 1.3524\nEpoch [12/20], Val Loss: 1.3057\nEpoch [13/20], Batch [10/40], Loss: 0.9205\nEpoch [13/20], Batch [20/40], Loss: 1.8696\nEpoch [13/20], Batch [30/40], Loss: 1.6219\nEpoch [13/20], Batch [40/40], Loss: 1.6180\nEpoch [13/20], Average Loss: 1.5731\nEpoch [13/20], Val Loss: 1.3678\nEpoch [14/20], Batch [10/40], Loss: 1.1716\nEpoch [14/20], Batch [20/40], Loss: 1.1421\nEpoch [14/20], Batch [30/40], Loss: 1.5288\nEpoch [14/20], Batch [40/40], Loss: 1.4980\nEpoch [14/20], Average Loss: 1.4261\nEpoch [14/20], Val Loss: 1.4322\nEpoch [15/20], Batch [10/40], Loss: 1.2878\nEpoch [15/20], Batch [20/40], Loss: 1.4272\nEpoch [15/20], Batch [30/40], Loss: 1.3642\nEpoch [15/20], Batch [40/40], Loss: 1.3015\nEpoch [15/20], Average Loss: 1.3557\nEpoch [15/20], Val Loss: 1.3955\nEpoch [16/20], Batch [10/40], Loss: 1.3575\nEpoch [16/20], Batch [20/40], Loss: 1.3346\nEpoch [16/20], Batch [30/40], Loss: 1.3899\nEpoch [16/20], Batch [40/40], Loss: 1.5069\nEpoch [16/20], Average Loss: 1.2633\nEpoch [16/20], Val Loss: 1.2050\nEpoch [17/20], Batch [10/40], Loss: 1.4292\nEpoch [17/20], Batch [20/40], Loss: 9.0929\nEpoch [17/20], Batch [30/40], Loss: 1.7033\nEpoch [17/20], Batch [40/40], Loss: 1.6997\nEpoch [17/20], Average Loss: 1.7543\nEpoch [17/20], Val Loss: 1.6105\nEpoch [18/20], Batch [10/40], Loss: 1.5014\nEpoch [18/20], Batch [20/40], Loss: 1.3709\nEpoch [18/20], Batch [30/40], Loss: 1.4644\nEpoch [18/20], Batch [40/40], Loss: 1.2367\nEpoch [18/20], Average Loss: 1.4952\nEpoch [18/20], Val Loss: 1.3207\nEpoch [19/20], Batch [10/40], Loss: 1.2997\nEpoch [19/20], Batch [20/40], Loss: 1.4969\nEpoch [19/20], Batch [30/40], Loss: 1.3156\nEpoch [19/20], Batch [40/40], Loss: 1.3230\nEpoch [19/20], Average Loss: 1.4062\nEpoch [19/20], Val Loss: 1.3236\nEpoch [20/20], Batch [10/40], Loss: 1.0311\nEpoch [20/20], Batch [20/40], Loss: 1.3576\nEpoch [20/20], Batch [30/40], Loss: 1.2166\nEpoch [20/20], Batch [40/40], Loss: 1.2335\nEpoch [20/20], Average Loss: 1.3189\nEpoch [20/20], Val Loss: 1.1977\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nunet_model.eval()\nwith torch.no_grad():\n    for i, sample in enumerate(test_loader):\n        inputs = sample['img'].to(device)\n        labels = sample['label'].to(device)\n\n        # Forward pass\n        outputs = unet_model(inputs)\n\n        for j in range(inputs.shape[0]):  # Iterate over the batch dimension\n            input_image = inputs[j].cpu().numpy()\n            output_image = outputs[j].argmax(dim=0).cpu().numpy()  # Convert the predicted output to class indices\n            label_image = labels[j].argmax(dim=0).cpu().numpy()  # Convert the ground truth labels to class indices\n\n            input_image = np.transpose(input_image, (1, 2, 0))\n\n            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n            axes[0].imshow(input_image)\n            axes[0].set_title('Original Image')\n            axes[0].axis('off')\n\n            axes[1].imshow(output_image, cmap='jet')\n            axes[1].set_title('Predicted Mask')\n            axes[1].axis('off')\n\n            axes[2].imshow(label_image, cmap='jet')\n            axes[2].set_title('Ground Truth')\n            axes[2].axis('off')\n\n            plt.tight_layout()\n            plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef calculate_iou(output_image, input_image):\n    intersection = np.logical_and(output_image, input_image)\n    union = np.logical_or(output_image, input_image)\n    iou_score = np.sum(intersection) / np.sum(union)\n    print(iou_score)\n\ndef calculate_pixel_accuracy(output_image, input_image):\n    correct_pixels = np.sum(output_image == input_image)\n    total_pixels = output_image.size\n    pixel_accuracy = correct_pixels / total_pixels\n    print(pixel_accuracy)\n\ndef calculate_miou(output_images, input_images):\n    miou_scores = []\n    for i in range(len(output_images)):\n        iou_score = calculate_iou(output_images[i], input_images[i])\n        if iou_score is not None:  # Check for None values\n            miou_scores.append(iou_score)\n    if miou_scores:  # Check if miou_scores is not empty\n        miou = np.mean(miou_scores)\n        print(miou)\n    else:\n        print(\"No valid IoU scores found.\")\n\n\n\ncalculate_iou(output_image, label_image)\nprint(\"*\"*50)\ncalculate_pixel_accuracy(output_image, label_image)\nprint(\"*\"*50)\ncalculate_miou(output_image, label_image)\n\n0.9997100830078125\n**************************************************\n0.8454132080078125\n**************************************************\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n0.99609375\n1.0\n0.99609375\n1.0\n0.99609375\n1.0\n0.984375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nNo valid IoU scores found."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "shreyansjain04.github.io",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]